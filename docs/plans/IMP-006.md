# IMP-006 — Signal journal + outcome tracking

- **Status:** done
- **Effort:** M
- **Alpha potential:** High

## Summary
Add a built-in journaling system that logs the tool’s daily signals (advice/actions, confluence scores, key metrics) and then automatically evaluates outcomes (next day / next week) using subsequent candle closes and snapshot marks where available.

Without this feedback loop, rule changes are “vibes” not measurement.

## Goals
- Persist daily “signal events” for:
  - positions (`analyze`/`advice` outputs)
  - research recommendations
  - scanner shortlists
- Persist “outcomes” at fixed horizons (e.g., +1D, +5D, +20D):
  - underlying return
  - option mark change (if snapshot history exists)
- Provide summary reports:
  - hit rates by signal type
  - distribution of outcomes
  - top/bottom performing rules

## Non-goals
- A full trading simulator (that’s IMP-007).
- Broker fills/transaction cost accounting beyond basic spread penalties.

## User-facing changes
- New commands:
  - `options-helper journal log portfolio.json --as-of latest`
  - `options-helper journal evaluate --window 252`
- New data directory (gitignored):
  - `data/journal/`

## Design
### Data model
- `SignalEvent` (JSONL):
  - date, symbol, context (`position|research|scanner`)
  - signal payload (action, score, reasons, warnings)
  - snapshot refs (snapshot_date, contractSymbol)
- `Outcome` (derived):
  - horizons (1/5/20)
  - underlying returns
  - option mark returns (if possible)

Prefer append-only JSONL for auditability.

### Outcome sourcing
- Underlying: candle cache closes.
- Options mark:
  - use snapshot mark for the same contractSymbol on the horizon date
  - if missing, leave `None` (don’t invent)

## Implementation plan
### Step 1 — Create a journal store
**Files:**
- `options_helper/data/journal.py` (new)

**Work:**
- Append/read JSONL
- Basic indexing helpers (by symbol/date/context)

### Step 2 — Implement `journal log`
**Files:**
- `options_helper/cli.py`

**Work:**
- Reuse existing computations (don’t recompute differently).
- Log:
  - analyze/advice for each position
  - research recommendations (top N)
  - scanner shortlist (top N)

### Step 3 — Implement `journal evaluate`
**Files:**
- `options_helper/analysis/journal_eval.py` (new)

**Work:**
- For each event, compute outcomes at configured horizons.
- Aggregate metrics into a deterministic JSON + Markdown report under `data/reports/journal/`.

### Step 4 — Docs
**Files:**
- `docs/JOURNAL.md` (new)

### Step 5 — Tests
**Files:**
- `tests/test_journal.py` (new)

**Work:**
- Create synthetic candle series and synthetic snapshot marks.
- Verify outcomes and report aggregation.

## Acceptance criteria
- Daily pipeline can optionally run `journal log` after briefing/analyze.
- `journal evaluate` produces a report that helps compare rule changes over time.
