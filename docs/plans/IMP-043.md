# IMP-043 — Options snapshot lake (Parquet) + DuckDB index

- **Status:** draft
- **Effort:** M–L
- **Alpha potential:** Very High

## Summary
Store **full option-chain snapshots** efficiently using:
- Parquet files (facts) partitioned by `symbol/snapshot_date/`
- DuckDB tables (headers/index) to track availability + metadata

This replaces the current “one CSV per expiry” layout when `--storage duckdb` is enabled, while
keeping filesystem mode unchanged.

## Goals
- Add `save_day_snapshot()` to the filesystem `OptionsSnapshotStore` (so snapshotter has a unified API).
- Implement `DuckDBOptionsSnapshotStore`:
  - `save_day_snapshot()` writes Parquet + header row in DuckDB
  - `load_day()` reads Parquet quickly via DuckDB
  - `list_dates()` / `resolve_date()` work without scanning the filesystem
- Refactor `snapshot_full_chain_for_symbols()` to call `save_day_snapshot()`.

## Non-goals
- Intraday options quotes/trades/bars (future work).
- Strategy/backtest changes (they just benefit from better storage).

## User-facing changes
- With `--storage duckdb`, snapshotting creates:
  - `data/warehouse/options.duckdb`
  - `data/options_snapshots/{SYMBOL}/{YYYY-MM-DD}/chain.parquet`
  - `.../meta.json.gz` and `.../raw.json.gz` (best-effort)

## Design
### Data model
- `options_snapshot_headers(symbol, snapshot_date, provider)` is the “inventory” table.
- Parquet fact file contains all calls+puts rows for the day (normalized with `expiry` + `optionType`).

### Provider dimension
- Key headers by provider name so multiple providers can be stored in parallel.

## Implementation plan
### Step 1 — Add unified API to filesystem snapshot store
**Files:**
- `options_helper/data/options_snapshots.py`

**Work:**
- Implement `save_day_snapshot()` by writing the per-expiry CSVs + raw payloads + meta.json.

### Step 2 — Implement DuckDB snapshot store
**Files:**
- `options_helper/data/stores_duckdb.py`

**Work:**
- Implement `DuckDBOptionsSnapshotStore` with:
  - header upsert in DuckDB
  - Parquet write via DuckDB `COPY`
  - Parquet read via DuckDB `read_parquet`

### Step 3 — Refactor snapshotter to use save_day_snapshot()
**Files:**
- `options_helper/data/options_snapshotter.py`

**Work:**
- Build full-day `chain_df` and `raw_by_expiry` and call `store.save_day_snapshot()` once per symbol/day.

### Step 4 — Route store via factory + update CLI
**Files:**
- `options_helper/data/store_factory.py`
- `options_helper/cli.py`

**Work:**
- Replace `OptionsSnapshotStore(...)` instantiation with factory.

## Tests
- `test_duckdb_options_snapshot_store.py`: save/load day snapshot and list_dates.
- Extend existing snapshot store tests to cover `save_day_snapshot()` (filesystem).

## Acceptance criteria
- Snapshotting many symbols does not create “thousands of CSV files” in DuckDB mode.
- Loading a snapshot day for a symbol remains a single DataFrame call.
- Existing filesystem mode snapshots remain readable.
